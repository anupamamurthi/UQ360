{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured data predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will do the following\n",
    "\n",
    "Use a simple data set (Sloan Digital Sky Survey) https://www.kaggle.com/lucidlenn/sloan-digital-sky-survey and banking data set https://www.kaggle.com/jonathanlicis/bank-marketing-data-set\n",
    "Create two types of splits. Every split will create three types of data - training data, testing data and productiond data\n",
    "- Random split\n",
    "- Biased split\n",
    "\n",
    "Biased split are used to introduce some drift\n",
    "\n",
    "- Train a Scikit learn model on the train data. Verify on test data and also look at the performance of the model on the production data\n",
    "- Fit a structured data predictor using the trained model, train/test data\n",
    "- Calculate the predictions as returned by the predictor on the production data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/anupamamurthi/Documents/GitHub/UQ360/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the CSV dataset from https://www.kaggle.com/lucidlenn/sloan-digital-sky-survey and place it in your local disk.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Split using Sloan Digital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_file = \"/Users/anupamamurthi/Downloads/sdss.csv\"\n",
    "columns = {\"ordered_categorical_columns\": [],\n",
    "        \"categorical_columns\": [],\n",
    "        \"numerical_columns\": ['ra', 'dec', 'u', 'g', 'r', 'i', 'z', 'run', 'camcol', 'field', \n",
    "                    'specobjid', 'redshift', 'plate', 'mjd', 'fiberid'],\n",
    "        \"text_columns\": [],\n",
    "        \"ignore_columns\": ['objid', 'rerun'],\n",
    "        \"targets\": 'class'}\n",
    "\n",
    "has_header = True\n",
    "separator = \",\"\n",
    "try:\n",
    "    if has_header:\n",
    "        df = pd.read_csv(local_file, sep=separator, na_values=[\"unknown\"], engine='python')\n",
    "    else:\n",
    "        df = pd.read_csv(local_file, sep=separator, na_values=[\"unknown\"], engine='python', header=None)\n",
    "except:\n",
    "    raise Exception(\"Cannot read local file: {}\".format(local_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>u</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>z</th>\n",
       "      <th>run</th>\n",
       "      <th>rerun</th>\n",
       "      <th>camcol</th>\n",
       "      <th>field</th>\n",
       "      <th>specobjid</th>\n",
       "      <th>class</th>\n",
       "      <th>redshift</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.531326</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>19.47406</td>\n",
       "      <td>17.04240</td>\n",
       "      <td>15.94699</td>\n",
       "      <td>15.50342</td>\n",
       "      <td>15.22531</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.722360e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.598370</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>18.66280</td>\n",
       "      <td>17.21449</td>\n",
       "      <td>16.67637</td>\n",
       "      <td>16.48922</td>\n",
       "      <td>16.39150</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>3.638140e+17</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>323</td>\n",
       "      <td>51615</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.680207</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>19.38298</td>\n",
       "      <td>18.19169</td>\n",
       "      <td>17.47428</td>\n",
       "      <td>17.08732</td>\n",
       "      <td>16.80125</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>268</td>\n",
       "      <td>3.232740e+17</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>287</td>\n",
       "      <td>52023</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.870529</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>17.76536</td>\n",
       "      <td>16.60272</td>\n",
       "      <td>16.16116</td>\n",
       "      <td>15.98233</td>\n",
       "      <td>15.90438</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.237650e+18</td>\n",
       "      <td>183.883288</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>17.55025</td>\n",
       "      <td>16.26342</td>\n",
       "      <td>16.43869</td>\n",
       "      <td>16.55492</td>\n",
       "      <td>16.61326</td>\n",
       "      <td>752</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>269</td>\n",
       "      <td>3.722370e+18</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>3306</td>\n",
       "      <td>54922</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          objid          ra       dec         u         g         r         i  \\\n",
       "0  1.237650e+18  183.531326  0.089693  19.47406  17.04240  15.94699  15.50342   \n",
       "1  1.237650e+18  183.598370  0.135285  18.66280  17.21449  16.67637  16.48922   \n",
       "2  1.237650e+18  183.680207  0.126185  19.38298  18.19169  17.47428  17.08732   \n",
       "3  1.237650e+18  183.870529  0.049911  17.76536  16.60272  16.16116  15.98233   \n",
       "4  1.237650e+18  183.883288  0.102557  17.55025  16.26342  16.43869  16.55492   \n",
       "\n",
       "          z  run  rerun  camcol  field     specobjid   class  redshift  plate  \\\n",
       "0  15.22531  752    301       4    267  3.722360e+18    STAR -0.000009   3306   \n",
       "1  16.39150  752    301       4    267  3.638140e+17    STAR -0.000055    323   \n",
       "2  16.80125  752    301       4    268  3.232740e+17  GALAXY  0.123111    287   \n",
       "3  15.90438  752    301       4    269  3.722370e+18    STAR -0.000111   3306   \n",
       "4  16.61326  752    301       4    269  3.722370e+18    STAR  0.000590   3306   \n",
       "\n",
       "     mjd  fiberid  \n",
       "0  54922      491  \n",
       "1  51615      541  \n",
       "2  52023      513  \n",
       "3  54922      510  \n",
       "4  54922      512  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after dropping a few:  ['ra', 'dec', 'u', 'g', 'r', 'i', 'z', 'run', 'camcol', 'field', 'specobjid', 'redshift', 'plate', 'mjd', 'fiberid']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = df.drop(columns.get('ignore_columns', []), axis=1)\n",
    "\n",
    "df = df.dropna(axis=0, how='any')\n",
    "\n",
    "# Get target value 'y'\n",
    "y_label = columns.get(\"targets\", 'y')\n",
    "y = df[y_label].values\n",
    "df = df.drop([y_label], axis=1)  # Remove labels from feature set x'\n",
    "\n",
    "\n",
    "task = \"classification\"\n",
    "if task == \"classification\":\n",
    "    class_dict = {}\n",
    "    for ind, label in enumerate(np.unique(y)):\n",
    "        class_dict[ind] = str(label)\n",
    "else:\n",
    "    class_dict = None\n",
    "# metadata['class_dict'] = class_dict\n",
    "\n",
    "\n",
    "# Encode labels as int\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "features = list(df.columns.values)\n",
    "print('Features after dropping a few: ', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random split with no drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test prod data\n",
    "\n",
    "def create_train_test_prod_split(x, y, test_size=0.25 ):\n",
    "    \"\"\"\n",
    "    returns x_train, y_train, x_test, y_test, x_prod, y_prod\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                                    test_size=0.25, \n",
    "                                                                random_state=42)\n",
    "\n",
    "    x_test, x_prod, y_test, y_prod = train_test_split(x_test, y_test,\n",
    "                                                                    test_size=0.25, \n",
    "                                                                random_state=42)\n",
    "\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_prod.shape, y_prod.shape)\n",
    "\n",
    "    \n",
    "    print(\"Training data size:\", x_train.shape)\n",
    "    print(\"Test data size:\", x_test.shape)\n",
    "    print(\"Prod data size:\", x_prod.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, x_prod, y_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x\n",
    "# features dictionary will represent the feature vector. \"x\" will contain one hot encoded data but \"features\" dict\n",
    "# will maintain the original structure, making it easy to filter and create biased splits.\n",
    "# x and features are both the same data but represented differently\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "metadata = {}\n",
    "# identify categorical vs numerical columns\n",
    "categorical_columns = columns.get(\"categorical_columns\", [])\n",
    "ordered_categorical_columns = columns.get(\"ordered_categorical_columns\", [])\n",
    "numerical_columns = columns.get(\"numerical_columns\", [])\n",
    "# use label encoder to convert string into numerical values\n",
    "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "if categorical_columns or ordered_categorical_columns:  # if there are categorical variables, store details\n",
    "    metadata['categorical_column_details'] = {}\n",
    "features = {}\n",
    "\n",
    "# feature_names = {}\n",
    "for column in df.columns.values:\n",
    "    if column in ordered_categorical_columns:\n",
    "        features[str(column)] = le.fit_transform(df[column].values.astype('str')).reshape(-1, 1)\n",
    "        metadata['categorical_column_details'][column] = list(le.classes_)\n",
    "    elif column in categorical_columns:\n",
    "        features[str(column)] = ohe.fit_transform(df[column].values.astype('str').reshape(-1, 1))\n",
    "    else:  # By default if not labeled as categorical, treat as numeric\n",
    "        features[str(column)] = df[column].values.astype('float').reshape(-1, 1)\n",
    "\n",
    "x = np.concatenate([np.array(v, dtype=float) for k, v in features.items()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 15) (7500,) (1875, 15) (1875,) (625, 15) (625,)\n",
      "Training data size: (7500, 15)\n",
      "Test data size: (1875, 15)\n",
      "Prod data size: (625, 15)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, x_prod, y_prod = create_train_test_prod_split(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test:  99.03999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_on_test = rf.score(x_test, y_test)\n",
    "print('Accuracy on test: ',acc_on_test*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on prod:  99.68\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_on_prod = rf.score(x_prod, y_prod)\n",
    "print('Accuracy on prod: ',acc_on_prod*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = ['Test', 'Prod']\n",
    "men_means = [round(acc_on_test,2)*100, round(acc_on_prod,2)*100]\n",
    "err = [0,0]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x , men_means, width, label='accuracy', yerr=err)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model performance on test and prod data')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend() \n",
    "\n",
    "def label(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "label(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features : ['confidence_top_distance', 'confidence_delta_distance', 'confidence_entropy_distance', 'random_forest_distance', 'gbm_distance', 'logistic_regression_distance', 'pca_distance', 'best_feature_distance', 'class_frequency_distance', 'num_important_features', 'bootstrap']\n",
      "Pointwise features : ['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm', 'class_frequency', 'random_forest', 'logistic_regression', 'one_class_svm', 'umap_kde']\n",
      "Blackbox features : ['pp_std', 'pp_entropy', 'pp_bootstrap', 'base_model_entropy_ratio', 'predicted_accuracy_change', 'pp_uncertainty']\n",
      "Predictor type : structured_data\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 15 TO 14\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 14 TO 13\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 13 TO 12\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 12 TO 11\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 11 TO 10\n",
      "\n",
      "UMAP KDE COULD NOT BE FIT WITHIN FIVE TRIES. RETURNING CONSTANT FEATURE. \n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'random_forest_1', 'random_forest_2', 'logistic_regression_1', 'logistic_regression_2', 'one_class_svm', 'umap_kde'])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/anupamamurthi/Documents/GitHub/UQ360/\")\n",
    "\n",
    "# It is possible to train the predictor using encoded data or using raw text. \n",
    "\n",
    "# In the below example, we are using raw text to train the predictor but x_train can be swapped with x_train_encoded\n",
    "\n",
    "from uq360.algorithms.blackbox_metamodel.structured_data_classification import StructuredDataClassificationWrapper\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "p = StructuredDataClassificationWrapper(base_model=rf)\n",
    "\n",
    "p.fit(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'random_forest_1', 'random_forest_2', 'logistic_regression_1', 'logistic_regression_2', 'one_class_svm', 'umap_kde'])\n"
     ]
    }
   ],
   "source": [
    "prediction, y_pred, y_score = p.predict(x_prod)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.11016264521925"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above use case, the model appears to perform well on both production and test data. Performance predictor predicts the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case 2: Biased split (Banking dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download banking dataset from https://www.kaggle.com/jonathanlicis/bank-marketing-data-set/activity and use it directly or use the compressed version available in data/structured_data/banking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "local_file = \"/Users/anupamamurthi/Documents/GitHub/UQ360/data/structured_data/banking/bank-additional-full.csv.gz\"\n",
    "\n",
    "file = {\"filename\":\"bank-additional-full.csv\", \"separator\":\";\", \"extension\":\".csv\"}\n",
    "columns = {\n",
    "    \"ordered_categorical_columns\": [\"education\"],\n",
    "        \"categorical_columns\": [\"job\", \"marital\", \"housing\", \"loan\", \"contact\", \"poutcome\"],\n",
    "        \"numerical_columns\": [\"age\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"],\n",
    "        \"text_columns\": [],\n",
    "        \"ignore_columns\": [\"default\",\"month\",\"day_of_week\"],\n",
    "        \"targets\": \"y\"}\n",
    "separator = file.get('separator', ',')\n",
    "\n",
    "has_header = True\n",
    "try:\n",
    "    if has_header:\n",
    "        df = pd.read_csv(local_file, sep=separator, na_values=[\"unknown\"], engine='python', compression=\"gzip\")\n",
    "    else:\n",
    "        df = pd.read_csv(local_file, sep=separator, na_values=[\"unknown\"], engine='python', header=None, compression=\"gzip\")\n",
    "except:\n",
    "    raise Exception(\"Cannot read local file: {}\".format(local_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education default housing loan    contact month  \\\n",
       "0   56  housemaid  married     basic.4y      no      no   no  telephone   may   \n",
       "1   57   services  married  high.school     NaN      no   no  telephone   may   \n",
       "2   37   services  married  high.school      no     yes   no  telephone   may   \n",
       "3   40     admin.  married     basic.6y      no      no   no  telephone   may   \n",
       "4   56   services  married  high.school      no      no  yes  telephone   may   \n",
       "\n",
       "  day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clean up\n",
    "import numpy as np\n",
    "df = df.drop(columns.get('ignore_columns', []), axis=1)\n",
    "\n",
    "df = df.dropna(axis=0, how='any')\n",
    "\n",
    "# Get target value 'y'\n",
    "y_label = columns.get(\"targets\", 'y')\n",
    "y = df[y_label].values\n",
    "df = df.drop([y_label], axis=1)  # Remove labels from feature set x'\n",
    "\n",
    "task = \"classification\"\n",
    "if task == \"classification\":\n",
    "    class_dict = {}\n",
    "    for ind, label in enumerate(np.unique(y)):\n",
    "        class_dict[ind] = str(label)\n",
    "else:\n",
    "    class_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after dropping a few:  ['age', 'job', 'marital', 'education', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels as int\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "features = list(df.columns.values)\n",
    "print('Features after dropping a few: ', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features dictionary will represent the feature vector. \"x\" will contain one hot encoded data but \"features\" dict\n",
    "# will maintain the original structure, making it easy to filter and create biased splits.\n",
    "# x and features are both the same data but represented differently\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "metadata = {}\n",
    "# identify categorical vs numerical columns\n",
    "categorical_columns = columns.get(\"categorical_columns\", [])\n",
    "ordered_categorical_columns = columns.get(\"ordered_categorical_columns\", [])\n",
    "numerical_columns = columns.get(\"numerical_columns\", [])\n",
    "# use label encoder to convert string into numerical values\n",
    "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "if categorical_columns or ordered_categorical_columns:  # if there are categorical variables, store details\n",
    "    metadata['categorical_column_details'] = {}\n",
    "features = {}\n",
    "\n",
    "# feature_names = {}\n",
    "for column in df.columns.values:\n",
    "    if column in ordered_categorical_columns:\n",
    "        features[str(column)] = le.fit_transform(df[column].values.astype('str')).reshape(-1, 1)\n",
    "        metadata['categorical_column_details'][column] = list(le.classes_)\n",
    "    elif column in categorical_columns:\n",
    "        features[str(column)] = ohe.fit_transform(df[column].values.astype('str').reshape(-1, 1))\n",
    "    else:  # By default if not labeled as categorical, treat as numeric\n",
    "        features[str(column)] = df[column].values.astype('float').reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x\n",
    "x = np.concatenate([np.array(v, dtype=float) for k, v in features.items()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38245, 34) (38245,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28683, 34) (28683,) (7171, 34) (7171,) (2391, 34) (2391,)\n",
      "Training data size: (28683, 34)\n",
      "Test data size: (7171, 34)\n",
      "Prod data size: (2391, 34)\n",
      "Accuracy on test:  91.35406498396318\n",
      "Accuracy on prod:  91.50982852363028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create a random train/test/prod split\n",
    "x_train, y_train, x_test, y_test, x_prod, y_prod = create_train_test_prod_split(x, y)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "acc_on_test = rf.score(x_test, y_test)\n",
    "print('Accuracy on test: ',acc_on_test*100)\n",
    "acc_on_prod = rf.score(x_prod, y_prod)\n",
    "print('Accuracy on prod: ',acc_on_prod*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features : ['confidence_top_distance', 'confidence_delta_distance', 'confidence_entropy_distance', 'random_forest_distance', 'gbm_distance', 'logistic_regression_distance', 'pca_distance', 'best_feature_distance', 'class_frequency_distance', 'num_important_features', 'bootstrap']\n",
      "Pointwise features : ['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm', 'class_frequency', 'random_forest', 'logistic_regression', 'one_class_svm', 'umap_kde']\n",
      "Blackbox features : ['pp_std', 'pp_entropy', 'pp_bootstrap', 'base_model_entropy_ratio', 'predicted_accuracy_change', 'pp_uncertainty']\n",
      "Predictor type : structured_data\n",
      "\n",
      "UMAP KDE: DOWNSAMPLING FROM 28683 TO 20000 SAMPLES\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 34 TO 33\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 33 TO 32\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 32 TO 31\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 31 TO 30\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 30 TO 29\n",
      "\n",
      "UMAP KDE COULD NOT BE FIT WITHIN FIVE TRIES. RETURNING CONSTANT FEATURE. \n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'random_forest_1', 'random_forest_2', 'logistic_regression_1', 'logistic_regression_2', 'one_class_svm', 'umap_kde'])\n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'random_forest_1', 'random_forest_2', 'logistic_regression_1', 'logistic_regression_2', 'one_class_svm', 'umap_kde'])\n",
      "91.18246932848658\n"
     ]
    }
   ],
   "source": [
    "from uq360.algorithms.blackbox_metamodel.structured_data_classification import StructuredDataClassificationWrapper\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "p1 = StructuredDataClassificationWrapper(base_model=rf)\n",
    "p1.fit(x_train, y_train, x_test, y_test)\n",
    "prediction, y_pred, y_score  = p1.predict(x_prod)  \n",
    "\n",
    "# There is no drift and the predictor's prediction matches\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few lines represent a recipe to generate a biased split. In the below example, we consider the feature \"nr.employed\" and consider two buckets\n",
    "\n",
    "- number of employees <= 5150 (bucket 0)\n",
    "- number of employees > 5150 (bucket 1)\n",
    "\n",
    "Three types of data will be generated \n",
    "\n",
    "- Training data (65%)\n",
    "- Test data (20%)\n",
    "- Production data (pool/log data) (10%)\n",
    "\n",
    "When generating training and test data, we grab 10% of the data from bucket 0 and 90% of the data from bucket 1. This would mean, the model will learn more about bucket 1.\n",
    "In like manner, when generating \"production data\", we grab 90% of the data from bucket 0 and 10% of the data from bucket 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata =  {\"buckets\": [{\"key\": \"nr.employed\", \"type\": \"feature\", \"condition\": \"lambda x, y: x if y <= 5150  else None\", \"name\": \"bucket_0_less_than_5090\"},\n",
    "                         {\"key\": \"nr.employed\", \"type\": \"feature\", \"condition\": \"lambda x, y: x if y > 5150 else None\", \"name\": \"bucket_1_more_than_5090\"}], \n",
    "             \"bucketizer\": {\"name\": \"nr_employees_based_split_predict_y\", \n",
    "                            \"x\": \"x\", \n",
    "                            \"train_test_prod\": [0.65, 0.25, 0.1], \n",
    "                            \"bias_buckets\": [\"bucket_0_less_than_5090\", \"bucket_1_more_than_5090\"], \n",
    "                            \"y\": \"y\", \n",
    "                            \"batches\": [{\"name\": \"batch_4\", \"bias_size\": [[0.1, 0.9], [0.1, 0.9], [0.9, 0.1]]}]\n",
    "                           }\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "name = \"biased_split\"\n",
    "logger = logging.getLogger(name)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    filename=None)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "split_config = metadata\n",
    "splits = []\n",
    "tmp_locations = []\n",
    "split=split_config\n",
    "\n",
    "if 'bias_buckets' not in split['bucketizer']:\n",
    "    raise Exception('bias_buckets is not specified! Bailing out')\n",
    "\n",
    "if 'batches' not in split['bucketizer']:\n",
    "    raise Exception('batches is not specified. There should atleast be one batch! bailing out..')\n",
    "\n",
    "batches = split['bucketizer']['batches']\n",
    "\n",
    "out_data = x\n",
    "out_labels = y\n",
    "biased_splits = []\n",
    "bias_buckets = split['bucketizer']['bias_buckets']\n",
    "bias_splits = {}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 22:53:08,235 - root - INFO -  $$$$$$$$$$$ handling biased buckets $$$$$$$$$$ \n",
      "2021-10-03 22:53:08,237 - root - INFO - handling bias bucket bucket_0_less_than_5090\n",
      "2021-10-03 22:53:08,306 - root - INFO - handling bias bucket bucket_1_more_than_5090\n"
     ]
    }
   ],
   "source": [
    "# move data into buckets\n",
    "import math\n",
    "if len(y) == 0 :\n",
    "    out_labels = features[split['bucketizer']['y']]\n",
    "\n",
    "logging.info(' $$$$$$$$$$$ handling biased buckets $$$$$$$$$$ ')\n",
    "bucket_detail = None\n",
    "for bias_bucket in bias_buckets:\n",
    "    logging.info('handling bias bucket %s', bias_bucket)\n",
    "    for bkt in split['buckets']:\n",
    "        if bkt['name'] == bias_bucket:\n",
    "            bucket_detail = bkt\n",
    "            break\n",
    "    # grab from features only if key is not y (which is the default)\n",
    "    out_labels_final = []\n",
    "    if 'key' in bucket_detail and bucket_detail['key'] != 'y':\n",
    "        out_labels_final = features.get(bucket_detail['key'], [])\n",
    "\n",
    "    condition = eval(bucket_detail['condition'])\n",
    "\n",
    "    if len(out_labels_final) != 0:\n",
    "        try:\n",
    "            output = list(map(condition, out_data, out_labels_final))\n",
    "        except ValueError as e:\n",
    "            # not the best approach! handling a scenario when the feature we split on is one-hot-encoded\n",
    "            print(e)\n",
    "            if str(e).startswith('The truth value of an array with more than one element is ambiguous'):\n",
    "                out_labels_final = [np.argmax(i) for i in out_labels_final]\n",
    "                output = list(map(condition, out_data, out_labels_final))\n",
    "    else:\n",
    "        output = list(map(condition, out_data, out_labels))\n",
    "    out1 = []\n",
    "    out2 = []\n",
    "    for index, item in enumerate(output):\n",
    "        if item is not None:\n",
    "            out1.append(item)\n",
    "            out2.append(out_labels[index])\n",
    "\n",
    "    out1 = np.array(out1)\n",
    "    out2 = np.array(out2)\n",
    "    bias_splits[bias_bucket] = {\"x\": out1, \"y\": out2, \"len\": len(out1)}\n",
    "\n",
    "\n",
    "train_dist = split['bucketizer']['train_test_prod'][0]\n",
    "test_dist = split['bucketizer']['train_test_prod'][1]\n",
    "prod_dist = split['bucketizer']['train_test_prod'][2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 22:53:08,989 - root - INFO -  ************************* Handling batch {'name': 'batch_4', 'bias_size': [[0.1, 0.9], [0.1, 0.9], [0.9, 0.1]]} ************************* \n",
      "2021-10-03 22:53:08,990 - root - INFO -  ###### per bucket distribution ##### \n",
      "2021-10-03 22:53:08,990 - root - INFO - train [7.000000000000001, 59.0]\n",
      "2021-10-03 22:53:08,991 - root - INFO - test [3.0, 23.0]\n",
      "2021-10-03 22:53:08,991 - root - INFO - prod [9.0, 1.0]\n",
      "2021-10-03 22:53:08,992 - root - INFO - magic number 310\n",
      "2021-10-03 22:53:08,992 - root - INFO - [2170.0000000000005, 18290.0] [930.0, 7130.0] [2790.0, 310.0]\n",
      "2021-10-03 22:53:08,993 - root - INFO - handling bucket...0\n",
      "2021-10-03 22:53:08,993 - root - INFO - 2170.0000000000005 930.0 2790.0\n",
      "2021-10-03 22:53:08,993 - root - INFO - create train_test and prod\n",
      "2021-10-03 22:53:08,994 - root - INFO - train_size 3100 test_size 2790\n",
      "2021-10-03 22:53:08,998 - root - INFO - handling bucket...1\n",
      "2021-10-03 22:53:08,999 - root - INFO - 18290.0 7130.0 310.0\n",
      "2021-10-03 22:53:08,999 - root - INFO - create train_test and prod\n",
      "2021-10-03 22:53:09,000 - root - INFO - train_size 25420 test_size 310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20460, 34) (20460,) (8060, 34) (8060,) (3100, 34) (3100,)\n"
     ]
    }
   ],
   "source": [
    "# Create a biased split by distributing the data from the two buckets into train, test, prod\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# start with min batch size\n",
    "batch_size = 100\n",
    "master_name = name\n",
    "\n",
    "for batch in batches:\n",
    "    logging.info(' ************************* Handling batch %s ************************* ', batch)\n",
    "    name = master_name # Reset name for each batch\n",
    "    try:\n",
    "        magic_number = None\n",
    "        \n",
    "        train = batch['bias_size'][0]\n",
    "        test = batch['bias_size'][1]\n",
    "        prod = batch['bias_size'][2]\n",
    "\n",
    "        dist = [0] * len(bias_buckets)  # size of buckets\n",
    "\n",
    "        # iterate thro all elements in train\n",
    "        train_dist_per_bucket = [0] * len(bias_buckets)\n",
    "        test_dist_per_bucket = [0] * len(bias_buckets)\n",
    "        prod_dist_per_bucket = [0] * len(bias_buckets)\n",
    "\n",
    "        for index, item in enumerate(train):\n",
    "            dist[index] = dist[index] + round(train_dist * item, 2)\n",
    "            train_dist_per_bucket[index] = round(train_dist * item, 2) * batch_size\n",
    "\n",
    "        for index, item in enumerate(test):\n",
    "            dist[index] = dist[index] + round(test_dist * item, 2)\n",
    "            test_dist_per_bucket[index] = round(test_dist * item, 2) * batch_size\n",
    "\n",
    "        for index, item in enumerate(prod):\n",
    "            dist[index] = dist[index] + round(prod_dist * item, 2)\n",
    "            prod_dist_per_bucket[index] = round(prod_dist * item, 2) * batch_size\n",
    "\n",
    "        logging.info(' ###### per bucket distribution ##### ')\n",
    "\n",
    "        logging.info('train %s' , train_dist_per_bucket)\n",
    "        logging.info('test %s' , test_dist_per_bucket)\n",
    "        logging.info('prod %s' , prod_dist_per_bucket)\n",
    "\n",
    "        # at this point dist will have len = number of buckets\n",
    "\n",
    "        dist = [i * batch_size for i in dist]\n",
    "\n",
    "        number_of_iterations = [0] * len(bias_buckets)\n",
    "        for index, bkt in enumerate(bias_buckets):\n",
    "            bkt_len = bias_splits[bkt]['len']\n",
    "            number_of_iterations[index] = math.floor(bkt_len / dist[index])\n",
    "\n",
    "        # magic number really determines the number of times you want to go around distributing the data\n",
    "        logging.info('magic number %s', min(number_of_iterations))\n",
    "        magic_number = min(number_of_iterations)\n",
    "\n",
    "        # train_dist_per_bucket\n",
    "        train_dist_per_bucket = [i * magic_number for i in train_dist_per_bucket]\n",
    "        test_dist_per_bucket = [i * magic_number for i in test_dist_per_bucket]\n",
    "        prod_dist_per_bucket = [i * magic_number for i in prod_dist_per_bucket]\n",
    "\n",
    "        logging.info(\"%s %s %s\",train_dist_per_bucket, test_dist_per_bucket, prod_dist_per_bucket)\n",
    "\n",
    "        # note, train_dist_per_bucket is of len = buckets\n",
    "        loop_index = 0\n",
    "\n",
    "        # we will loop thro each bucket now\n",
    "        x_train_final = []\n",
    "        y_train_final = []\n",
    "\n",
    "        x_test_final = []\n",
    "        y_test_final = []\n",
    "\n",
    "        x_prod_final = []\n",
    "        y_prod_final = []\n",
    "    \n",
    "        while True:\n",
    "            if loop_index >= len(bias_buckets):\n",
    "                break\n",
    "\n",
    "            bucket_name = bias_buckets[loop_index]\n",
    "\n",
    "            logging.info('handling bucket...%s', loop_index)\n",
    "            logging.info('%s %s %s',train_dist_per_bucket[loop_index], test_dist_per_bucket[loop_index], prod_dist_per_bucket[loop_index])\n",
    "\n",
    "            train_size = round(train_dist_per_bucket[loop_index] + test_dist_per_bucket[loop_index])\n",
    "            prod_size = round(prod_dist_per_bucket[loop_index])\n",
    "\n",
    "            logging.info('create train_test and prod')\n",
    "            logging.info('train_size %s test_size %s', train_size, prod_size)\n",
    "            if train_size == 0 and prod_size > 0:\n",
    "                x_train_test = np.array([])\n",
    "                y_train_test = np.array([])\n",
    "                x_prod = bias_splits[bucket_name][\"x\"]\n",
    "                y_prod = bias_splits[bucket_name][\"y\"]\n",
    "            elif prod_size == 0 and train_size > 0:\n",
    "                x_train_test = bias_splits[bucket_name][\"x\"]\n",
    "                y_train_test = bias_splits[bucket_name][\"y\"]\n",
    "                x_prod = np.array([])\n",
    "                y_prod = np.array([])\n",
    "            else:\n",
    "                x_train_test, x_prod, y_train_test, y_prod = train_test_split(bias_splits[bucket_name][\"x\"],\n",
    "                                                                            bias_splits[bucket_name][\"y\"],\n",
    "                                                                            train_size=train_size,\n",
    "                                                                            test_size=prod_size)\n",
    "\n",
    "            # edges 0,1 / 1,0 split\n",
    "            if len(x_train_test) != 0 and len(x_prod)!=0:\n",
    "                train_size = math.floor(train_dist_per_bucket[loop_index])\n",
    "                test_size = math.floor(test_dist_per_bucket[loop_index])\n",
    "                if train_size == 0 and test_size > 0:\n",
    "                    x_train = np.array([])\n",
    "                    y_train = np.array([])\n",
    "                    x_test = x_train_test\n",
    "                    y_test = y_train_test\n",
    "                elif train_size > 0 and test_size == 0:\n",
    "                    x_train = x_train_test\n",
    "                    y_train = y_train_test\n",
    "                    x_test = np.array([])\n",
    "                    y_test = np.array([])\n",
    "                else:\n",
    "                    x_train, x_test, y_train, y_test = train_test_split(x_train_test, y_train_test,\n",
    "                                                                            train_size = train_size,\n",
    "                                                                            test_size=test_size)\n",
    "                x_train_final.extend(x_train)\n",
    "                y_train_final.extend(y_train)\n",
    "\n",
    "                x_test_final.extend(x_test)\n",
    "                y_test_final.extend(y_test)\n",
    "\n",
    "                x_prod_final.extend(x_prod)\n",
    "                y_prod_final.extend(y_prod)\n",
    "\n",
    "            else:\n",
    "                if len(x_train_test) == 0:\n",
    "                    x_prod_new, x_throw, y_prod_new, y_throw = train_test_split(x_prod, y_prod,\n",
    "                                                                        train_size=math.floor(\n",
    "                                                                            prod_dist_per_bucket[loop_index]))\n",
    "                    x_prod_final.extend(x_prod_new)\n",
    "                    y_prod_final.extend(y_prod_new)\n",
    "                if len(x_prod) == 0:\n",
    "                    x_train, x_test, y_train, y_test = train_test_split(x_train_test, y_train_test,\n",
    "                                                                                train_size = math.floor(train_dist_per_bucket[loop_index]),\n",
    "                                                                                test_size=math.floor(test_dist_per_bucket[loop_index]))\n",
    "                    x_train_final.extend(x_train)\n",
    "                    y_train_final.extend(y_train)\n",
    "\n",
    "                    x_test_final.extend(x_test)\n",
    "                    y_test_final.extend(y_test)             \n",
    "            loop_index = loop_index + 1\n",
    "        \n",
    "        print(np.asarray(x_train_final).shape,np.asarray(y_train_final).shape,\n",
    "              np.asarray(x_test_final).shape,np.asarray(y_test_final).shape,\n",
    "              np.asarray(x_prod_final).shape,np.asarray(y_prod_final).shape)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy on test:  93.12655086848636\n",
      "RF Accuracy on prod:  81.54838709677419\n"
     ]
    }
   ],
   "source": [
    "# Train a simple RF model and check the accuracy of the model on train \n",
    "from uq360.algorithms.blackbox_metamodel.structured_data_classification import StructuredDataClassificationWrapper\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "rf1 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "               max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
    "               min_impurity_decrease=0.0, \n",
    "               min_samples_leaf=1, min_samples_split=2,\n",
    "               min_weight_fraction_leaf=0.0, n_estimators=576,\n",
    "               n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
    "               warm_start=False)\n",
    "\n",
    "rf1.fit(x_train_final, y_train_final)\n",
    "\n",
    "acc_on_test = rf1.score(x_test_final, y_test_final)\n",
    "print('RF Accuracy on test: ',acc_on_test*100)\n",
    "\n",
    "acc_on_prod = rf1.score(x_prod_final, y_prod_final)\n",
    "print('RF Accuracy on prod: ',acc_on_prod*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to be doing pretty well on test set but not so much on the production data. Fit the predictor on train/test data and perform a predict operation to learn about the model's performance on prod data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features : ['confidence_top_distance', 'confidence_delta_distance', 'confidence_entropy_distance', 'random_forest_distance', 'gbm_distance', 'logistic_regression_distance', 'pca_distance', 'best_feature_distance', 'class_frequency_distance', 'num_important_features', 'bootstrap']\n",
      "Pointwise features : ['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm', 'class_frequency', 'random_forest', 'logistic_regression', 'one_class_svm', 'umap_kde']\n",
      "Blackbox features : ['pp_std', 'pp_entropy', 'pp_bootstrap', 'base_model_entropy_ratio', 'predicted_accuracy_change', 'pp_uncertainty']\n",
      "Predictor type : structured_data\n",
      "\n",
      "UMAP KDE: DOWNSAMPLING FROM 20460 TO 20000 SAMPLES\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 34 TO 33\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 33 TO 32\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 32 TO 31\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 31 TO 30\n",
      "\n",
      "\n",
      "UMAP KDE: FIT-TRANSFORM FAILED. REDUCING DIMENSIONS WITH PCA FROM 30 TO 29\n",
      "\n",
      "UMAP KDE COULD NOT BE FIT WITHIN FIVE TRIES. RETURNING CONSTANT FEATURE. \n",
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'random_forest_1', 'random_forest_2', 'logistic_regression_1', 'logistic_regression_2', 'one_class_svm', 'umap_kde'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted for : odict_keys(['confidence_top', 'confidence_delta', 'confidence_entropy', 'gbm_1', 'gbm_2', 'class_frequency', 'random_forest_1', 'random_forest_2', 'logistic_regression_1', 'logistic_regression_2', 'one_class_svm', 'umap_kde'])\n",
      "Prediction 79.97449374344531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/anupamamurthi/Documents/GitHub/UQ360/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from uq360.algorithms.blackbox_metamodel.structured_data_classification import StructuredDataClassificationWrapper\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "p1 = StructuredDataClassificationWrapper(base_model=rf1)\n",
    "\n",
    "p1.fit(np.asarray(x_train_final), np.asarray(y_train_final), np.asarray(x_test_final), np.asarray(y_test_final))\n",
    "\n",
    "\n",
    "prediction, y_pred, y_score  = p1.predict(np.asarray(x_prod_final))  \n",
    "\n",
    "print(\"Prediction\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen about, the predictor predicts the model's accuracy to be around 80% in an extreme drift scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_uq360",
   "language": "python",
   "name": ".venv_uq360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
